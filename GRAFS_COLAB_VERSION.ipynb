{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1LTopfjvBrM2GxF2gG_5htUEZCBjcEpuS",
      "authorship_tag": "ABX9TyO/RcFKXBP4ndcuPkV7jlF+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/levilott/GRAFS/blob/main/GRAFS_COLAB_VERSION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generalized Robust Approach to Feature Selection (GRAFS)\n",
        "\n",
        "This notebook contains the source code and provides an example for the Generalized Robust Approach to Feature Selection documented here: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4494520\n",
        "\n",
        "This approach utilizes a Deep Neural Network (DNN) to identify salient features with or without a target variable. In the case no target variable (response) is identified, the method will perform k-means clustering to generate a target response and identify which features are salient in predicting the respective clusters.\n",
        "\n",
        "The DNN inputs and architecutre are inspired by a full-factorial designed experiment, a technique used in the Design of Experiments (DoE) field of study to characterize the impact of varying conditions (features) on a system's performance (response).\n",
        "\n",
        "This notebook is meant to be a stand-alone example performing feature selection for the Wisconsin Breast Cancer Diagnostic Dataset from the UCI ML Repo. We will pull the dataset from my gitlab for convienence which also hosts the additional materials on GRAFS.\n",
        "\n",
        "Simply execute each cell to see the demo."
      ],
      "metadata": {
        "id": "hGFaRsopbsHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build the functions\n",
        "\n",
        "The cell below builds GRAFS. Alternatively this cell may be downloaded as a .py file into your local directory and subsequently imported as is done with any other module."
      ],
      "metadata": {
        "id": "bm0sy5FUfizL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zI4UDYsECUZC"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Oct  3 17:44:01 2022\n",
        "\n",
        "@author: Bradford Lott\n",
        "\n",
        "RFE-GRAFS Feature Selection Implementation Master File\n",
        "\n",
        "This will be the front facing version on GitHub\n",
        "\"\"\"\n",
        "\n",
        "###################################################################\n",
        "\n",
        "\"\"\"INSERT PRE-PROCESSING FILE FROM THESIS\n",
        "   WILL NEED TO ADD RFE CAPABILITY IN THIS SECTION\"\"\"\n",
        "\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Oct  5 10:43:23 2021\n",
        "\n",
        "@author: Bradford Lott\n",
        "\n",
        "Pre-Processing for my thesis\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "###################################################################\n",
        "\n",
        "\"\"\"\n",
        "Created on Thu Sep 23 08:32:25 2021\n",
        "\n",
        "@author: Bradford Lott\n",
        "\n",
        "Lott Feature Selection Approach\n",
        "\"\"\"\n",
        "\n",
        "####################################################################\n",
        "\n",
        "\"TO DO:\"\n",
        "\n",
        "\"UPDATE NEW OHE NAMES WITH ORGIINAL VARIBLE NAME AS WELL IN ONE HOT ENCODER FUNCTION\"\n",
        "\n",
        "\"We have features and responses stored twice as dataframes and arrays, this is inefficient and could casue problems if we dont update both, just get it working for now and fix in future\"\n",
        "\n",
        "\"Currently we wont check for unique row ids, for example if someone includes id as a feature it wont automatically be dropped. Perhaps we should go back and check if a column only contains unique intigers or names.\"\n",
        "\n",
        "####################################################################\n",
        "\n",
        "\"Read in file.\"\n",
        "\n",
        "def read_data(filepath):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    filepath : Path to user data Must be csv, txt, or xls file.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dataframe with all variables (features and any response).\n",
        "\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    if 'csv' in filepath:\n",
        "        Full_Data = pd.read_csv(filepath)\n",
        "\n",
        "    elif 'txt' in filepath:\n",
        "            Full_Data = pd.read_csv(filepath)\n",
        "\n",
        "    elif 'xls' in filepath:\n",
        "        Full_Data = pd.read_excel(filepath)\n",
        "\n",
        "    else:\n",
        "        print(\"Please ensure you have provided a csv, txt, or xls file.\")\n",
        "\n",
        "    return Full_Data\n",
        "\n",
        "#####################################################################\n",
        "\"\"\"Check for response column. Seperate Features and Response.\"\"\"\n",
        "\n",
        "def check_response(Data,contains_response,**kwargs):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    Data : Full dataset as pandas dataframe with all variables (features and response)\n",
        "    contains_response : Bool, True if contains response else False\n",
        "        DESCRIPTION.\n",
        "    response_name : String, response column name\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Seperate dataframes for our feature data and response data\n",
        "\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    if contains_response == True:\n",
        "\n",
        "        response_name = kwargs.get('response_name', None)\n",
        "        Response_Data = pd.DataFrame(Data[response_name])\n",
        "        #make response column Y\n",
        "        #going to do this for both user input and fabricated responses\n",
        "        #the user will know what their response column will be called if it contains one\n",
        "        #need to update some doc strings to explain this though\n",
        "        Response_Data.columns = ['Y']\n",
        "        Feature_Data = pd.DataFrame(Data[Data.columns[Data.columns!=response_name]])\n",
        "\n",
        "        return Feature_Data , Response_Data , contains_response\n",
        "\n",
        "    else:\n",
        "        Feature_Data = Data\n",
        "        Response_Data = pd.DataFrame()\n",
        "        print(\"No response present. Fabricate categorical response using K-Means clustering.\")\n",
        "\n",
        "        return Feature_Data , Response_Data, contains_response\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "######################################################################\n",
        "\n",
        "def RFE(Feature_OHE_Data,Response_Data, perform_rfe,**kwargs):\n",
        "    \"\"\"\n",
        "\n",
        "    Need to update this doc string. This performs RFE and uses output as seed\n",
        "    to GRAFS if True.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.feature_selection import RFE\n",
        "    from sklearn.tree import DecisionTreeRegressor\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "    if perform_rfe == True:\n",
        "        rfe_retain = kwargs.get('rfe_retain', None)\n",
        "        rfe_retain = int(rfe_retain)\n",
        "        rfe_step = kwargs.get('rfe_step', 1)\n",
        "        rfe_step = int(rfe_step)\n",
        "\n",
        "        #categorical multi-category response\n",
        "        if Response_Data['Y'].dtype == 'int32' or Response_Data['Y'].dtype == 'int64' or Response_Data['Y'].dtype == 'object':\n",
        "            estimator = DecisionTreeClassifier()\n",
        "        else:\n",
        "            estimator = DecisionTreeRegressor()\n",
        "\n",
        "\n",
        "        selector = RFE(estimator, n_features_to_select=rfe_retain, step=rfe_step)\n",
        "        selector = selector.fit(Feature_OHE_Data, Response_Data)\n",
        "        selector.support_\n",
        "        selector.score(Feature_OHE_Data, Response_Data)\n",
        "        RFE_Features = list(Feature_OHE_Data.columns[selector.support_])\n",
        "\n",
        "        return RFE_Features\n",
        "\n",
        "    else:\n",
        "        RFE_Features = pd.DataFrame()\n",
        "        print(\"RFE not performed.\")\n",
        "\n",
        "        return RFE_Features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "######################################################################\n",
        "\n",
        "\n",
        "\"NEED TO UPDATE THIS TO INCLUDE THE VARIABLE NAME AS PART OF THE NEW OHE NAME\"\n",
        "\n",
        "\"Create One Hots for categorical Features. Can use this same function on Response.\"\n",
        "\n",
        "def one_hot_encoder(data):\n",
        "    \"\"\"\n",
        "    Checks whether a dataframe contains categorical data, then performs one-hot encoding on categorical features\n",
        "\n",
        "    Args:\n",
        "        data (pandas dataframe): Input dataframe\n",
        "\n",
        "    Returns:\n",
        "        data (pandas dataframe): Transformed dataframe with one-hot encoded categorical features\n",
        "\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    #get list of cols which are object data types (text data types)\n",
        "    categorical_cols = list(data.select_dtypes(include=['object']).columns)\n",
        "\n",
        "    #apply get dummies from pandas to do one hots over list of categorical features\n",
        "    one_hots = pd.DataFrame()\n",
        "    for i in categorical_cols:\n",
        "        hold = pd.get_dummies(data[i], prefix=f\"{i}_One_Hot\") #UPDATE WITH NAME HERE\n",
        "        one_hots = pd.concat([one_hots, hold], axis=1, ignore_index=False)\n",
        "\n",
        "    #cbind one hots to full dataset\n",
        "    data=pd.concat([data, one_hots], axis=1, ignore_index=False)\n",
        "\n",
        "    #drop cols with object data type\n",
        "    OHE_Data = (data.drop(columns=categorical_cols)).astype(float, errors = 'raise')\n",
        "\n",
        "    return OHE_Data\n",
        "\n",
        "######################################################################\n",
        "\n",
        "\"Check contains response. If False then perform K-means to fabricate categorical response.\"\n",
        "\n",
        "def fabricate_response(contains_response, RandomSeed, Standard_Feature_Array, lower_k, upper_k, n_init, max_iter, method):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    contains_response : Bool, True if the dataset contains a response taken from check response function\n",
        "\n",
        "    RandomSeed : Random seed to use in K-means\n",
        "\n",
        "    Standard_Feature_Array : Standardized Features\n",
        "\n",
        "    lower_k : Smallest number of groups to consider, should default this to 2\n",
        "\n",
        "    upper_k : Largest number of groups to consider, could default this to some function of the feature space size\n",
        "\n",
        "    n_init : number of initializations to try\n",
        "\n",
        "    max_iter : max iterations\n",
        "\n",
        "    method : string, random initialization or K++, \"random\", \"k-means++\"\n",
        "\n",
        "    Returns\n",
        "\n",
        "    fabricated response data\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if contains_response != True:\n",
        "\n",
        "        from sklearn.cluster import KMeans\n",
        "        from sklearn.metrics import silhouette_score\n",
        "        import pandas as pd\n",
        "\n",
        "        kmeans_kwargs = {\n",
        "        \"init\": method,\n",
        "        \"n_init\": n_init,\n",
        "        \"max_iter\": max_iter,\n",
        "        \"random_state\": RandomSeed,\n",
        "        }\n",
        "\n",
        "\n",
        "        silhouette_coefficients = []\n",
        "\n",
        "\n",
        "        for k in range(lower_k, upper_k):\n",
        "            kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
        "            kmeans.fit(Standard_Feature_Array)\n",
        "            score = silhouette_score(Standard_Feature_Array, kmeans.labels_)\n",
        "            silhouette_coefficients.append(score)\n",
        "\n",
        "        silhouette_coefficients = pd.DataFrame(silhouette_coefficients)\n",
        "\n",
        "        best_k = silhouette_coefficients[0].argmax() + lower_k\n",
        "\n",
        "        kmeans = KMeans(n_clusters = best_k, **kmeans_kwargs)\n",
        "        kmeans.fit(Standard_Feature_Array)\n",
        "\n",
        "        Fabricated_Response_Array = kmeans.labels_\n",
        "\n",
        "        Fabricated_Response_Array = Fabricated_Response_Array + 1 #add 1 so we dont have a \"0\" cluster just more intuitive to me\n",
        "\n",
        "        Fabricated_Response_Data = pd.DataFrame(Fabricated_Response_Array)\n",
        "\n",
        "        Fabricated_Response_Data.columns = ['Y']\n",
        "\n",
        "        return silhouette_coefficients, best_k, Fabricated_Response_Data\n",
        "\n",
        "######################################################################\n",
        "\n",
        "\"If categorical response, create stratified train/validation split based on response\"\n",
        "\"We have no need for a test set as we do not care about our models predictive performance\"\n",
        "\n",
        "#not clean to pass features as array and response as data\n",
        "#i dont like that im doing it this way and should go back and change this\n",
        "def splitData(Standard_Feature_Array, Response_Data):\n",
        "    \"\"\"\n",
        "    This function creates a test/validation split. It stratifies by response type if categorical.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    Standard_Feature_Data : standardized feature data frame\n",
        "    Response_Data : response data frame\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Balanced test/validation split based on response type if categorical.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    if Response_Data['Y'].dtype == 'int32' or Response_Data['Y'].dtype == 'int64' or Response_Data['Y'].dtype == 'object':\n",
        "        x_train, x_val, y_train, y_val = train_test_split(Standard_Feature_Array, Response_Data,\n",
        "                                                          test_size=0.4,\n",
        "                                                          stratify=np.array(Response_Data),\n",
        "                                                          random_state=1234)\n",
        "    else:\n",
        "        x_train, x_val, y_train, y_val = train_test_split(Standard_Feature_Array, Response_Data,\n",
        "                                                          test_size=0.4,\n",
        "                                                          shuffle = False,\n",
        "                                                          stratify = None,\n",
        "                                                          random_state=1234)\n",
        "\n",
        "    Train_Feature_Data = pd.DataFrame(x_train)\n",
        "    Val_Feature_Data = pd.DataFrame(x_val)\n",
        "\n",
        "    Train_Response_Data = pd.DataFrame(y_train)\n",
        "    Val_Response_Data = pd.DataFrame(y_val)\n",
        "\n",
        "    return Train_Feature_Data, Val_Feature_Data, Train_Response_Data, Val_Response_Data\n",
        "\n",
        "\n",
        "######################################################################\n",
        "\n",
        "\"\"\"\n",
        "Created on Thu Sep 23 11:12:47 2021\n",
        "\n",
        "@author: Bradford Lott\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def FSPreProcessing(filepath, contains_response, perform_rfe, **kwargs):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    REQUIRED:\n",
        "\n",
        "    filepath : String, path to datafile\n",
        "\n",
        "    contains_response : Bool, True if dataset contains a response otherwise False\n",
        "\n",
        "    perform_rfe : Bool, if true we will use RFE output to seed GRAFS approach\n",
        "\n",
        "    OPTIONAL:\n",
        "\n",
        "    response_name: String, name of response column\n",
        "\n",
        "    rfe_retain : int, user presribed number of features for RFE to retain\n",
        "\n",
        "    rfe_step : int, number of features to drop per step in RFE\n",
        "\n",
        "    RETURNS:\n",
        "\n",
        "    Full_Data:\n",
        "\n",
        "    Train_Feature_Data:\n",
        "\n",
        "    Val_Feature_Data:\n",
        "\n",
        "    Train_Response_Data:\n",
        "\n",
        "    Val_Response_Data:\n",
        "\n",
        "    Train_Response_Vector:\n",
        "\n",
        "    Val_Response_Vector:\n",
        "\n",
        "\n",
        "    -------\n",
        "\n",
        "    This funciton will perform all pre-processing steps to format data for our FS algorithm.\n",
        "    These steps include:\n",
        "\n",
        "        Read in data from the provided file path\n",
        "\n",
        "        If the dataset does not contain a response perform K-means++ to fabricate a response...\n",
        "        ...choosing K-clusters where K is the best K based on silhouette score\n",
        "\n",
        "        Create one-hot encodings for categorical features and responses\n",
        "\n",
        "        Standardize our feature space\n",
        "\n",
        "        Create train and validation splits, stratified by response if response is categorical\n",
        "\n",
        "        Format our response data as vector for our NN\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    response_name = kwargs.get('response_name', None)\n",
        "    rfe_retain = kwargs.get('rfe_retain', 12)\n",
        "    rfe_retain = int(rfe_retain)\n",
        "    rfe_step = kwargs.get('rfe_step', 1)\n",
        "    rfe_step = int(rfe_step)\n",
        "\n",
        "    #import our functions file\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    #read data funciton from FS Functions\n",
        "    Full_Data = read_data(filepath)\n",
        "\n",
        "    #check response and seperate response and features\n",
        "    Feature_Data, Response_Data, contains_response = check_response(Full_Data, contains_response = contains_response, response_name = response_name)\n",
        "\n",
        "    #one hot encode categorical features\n",
        "    Feature_OHE_Data = one_hot_encoder(Feature_Data)\n",
        "    #this will return one hot encoded data as OHE_Data\n",
        "\n",
        "    #perform RFE here if desired and return as Feature_OHE_Data to maintain\n",
        "    #naming convetion for easy integration into rest of the code\n",
        "\n",
        "    if perform_rfe == True:\n",
        "        RFE_Features = RFE(Feature_OHE_Data, Response_Data, perform_rfe = perform_rfe, rfe_retain = rfe_retain, rfe_step = rfe_step)\n",
        "        Feature_OHE_Data = Feature_OHE_Data[RFE_Features]\n",
        "\n",
        "    #Standardize Feature Data\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    Standard_MainEffects_Array = scaler.fit_transform(Feature_OHE_Data)\n",
        "\n",
        "\n",
        "    #if contains_response from check response == True then we already have our resposne column\n",
        "    #otherwise we need to fabricate a categorical response from K-means clustering\n",
        "    if contains_response != True:\n",
        "       silhouette_coefficients, best_k, Response_Data = fabricate_response(contains_response = contains_response, RandomSeed = 1234, Standard_Feature_Array = Standard_MainEffects_Array, lower_k = 2, upper_k = 11, n_init = 10, max_iter = 300, method = \"k-means++\")\n",
        "\n",
        "\n",
        "    ################################################################3\n",
        "    \"\"\"INSERT INTERACTIONS AND CORRELATIONS CALCULATIONS\"\"\"\n",
        "\n",
        "    from itertools import combinations\n",
        "    #establish all as empty dataframes\n",
        "    Interactions_Data = pd.DataFrame()\n",
        "\n",
        "    Interaction_Terms = pd.DataFrame()\n",
        "    Main_Terms = pd.DataFrame()\n",
        "    Square_Terms = pd.DataFrame()\n",
        "\n",
        "    #capture interaction data for all interactions\n",
        "    for col1, col2 in combinations(Feature_OHE_Data.columns, 2):\n",
        "        Interactions_Data[f\"{col1}_{col2}\"] = Feature_OHE_Data[col1] * Feature_OHE_Data[col2]\n",
        "\n",
        "    #capture interaction term names, type, main effects\n",
        "    Interaction_Terms['Feature'] = Interactions_Data.columns\n",
        "    Interaction_Terms['Type'] = \"Interaction\"\n",
        "\n",
        "    Effect_List = []\n",
        "    for col1, col2 in combinations(Feature_OHE_Data.columns, 2):\n",
        "        Effect_List.append([f\"{col1}\", f\"{col2}\"])\n",
        "\n",
        "    Interaction_Terms['MainEffects'] = Effect_List\n",
        "\n",
        "    #capture main effect and squared data\n",
        "    for col in Feature_OHE_Data.columns:\n",
        "        Interactions_Data[f\"{col}\"] = Feature_OHE_Data[col]\n",
        "        Interactions_Data[f\"{col}_{col}\"] = Feature_OHE_Data[col]**2\n",
        "\n",
        "    #capture response\n",
        "    Interactions_Data['Y'] = Response_Data['Y']\n",
        "\n",
        "\n",
        "    #capture main effect term names, type, main effects\n",
        "    Main_Terms['Feature'] = Feature_OHE_Data.columns\n",
        "    Main_Terms['Type'] = \"Main\"\n",
        "\n",
        "    Effect_List = []\n",
        "    for name in Feature_OHE_Data.columns:\n",
        "        Effect_List.append([name])\n",
        "\n",
        "    Main_Terms['MainEffects'] =  Effect_List\n",
        "\n",
        "    #capture square term names, type, main effects\n",
        "    Effect_List = []\n",
        "    for name in Feature_OHE_Data.columns:\n",
        "        Effect_List.append(f\"{name}_{name}\")\n",
        "    Square_Terms['Feature'] = Effect_List\n",
        "    Square_Terms['Type'] = \"Square\"\n",
        "\n",
        "    #not efficient to do this over but it works for now\n",
        "    #could assign based on previous dataframe but dont want to for now\n",
        "\n",
        "    Effect_List = []\n",
        "    for name in Feature_OHE_Data.columns:\n",
        "        Effect_List.append([name])\n",
        "\n",
        "    Square_Terms['MainEffects'] = Effect_List\n",
        "\n",
        "    All_Terms = pd.concat([Main_Terms, Interaction_Terms, Square_Terms], axis = 0, ignore_index = True)\n",
        "    All_Terms.reset_index(drop = True, inplace = True)\n",
        "\n",
        "    #Interactions_Data.to_csv('Interactions_Test.csv', index = False)\n",
        "\n",
        "    #make our correlations for numeric response\n",
        "    if Response_Data['Y'].dtype == 'float64':\n",
        "        Correlations = pd.DataFrame(Interactions_Data[Interactions_Data.columns[0:]].corr()['Y'])\n",
        "        Correlations.reset_index(level = 0, inplace = True)\n",
        "        Correlations.columns = ['Feature', 'Corr']\n",
        "        drop_rows = Correlations[ Correlations['Feature'] == 'Y' ].index\n",
        "        Correlations.drop(drop_rows, inplace = True)\n",
        "        Correlations['Corr'] = Correlations['Corr'].abs()\n",
        "    else:\n",
        "        Interactions_Corr = one_hot_encoder(Interactions_Data)\n",
        "        response_level_count = len(Response_Data['Y'].unique())\n",
        "        HoldCorrs = pd.DataFrame(Interactions_Corr[Interactions_Corr.columns[0:]].corr().iloc[:,-response_level_count:])\n",
        "        HoldCorrs.drop(HoldCorrs.tail(response_level_count).index,inplace = True)\n",
        "        HoldCorrs = HoldCorrs.abs()\n",
        "        Correlations = pd.DataFrame()\n",
        "        Correlations['Corr'] = HoldCorrs.max(axis=1)\n",
        "        Correlations.reset_index(level = 0, inplace = True)\n",
        "        Correlations.columns = ['Feature', 'Corr']\n",
        "\n",
        "\n",
        "    \"\"\"Correlations for one hots from the same variable appear to be NaN which works out well.\n",
        "    We can drop these\"\"\"\n",
        "\n",
        "    Correlations = Correlations.dropna(axis= 0)\n",
        "\n",
        "    \"\"\"Need to sort next\"\"\"\n",
        "\n",
        "    \"\"\"Add correlation of all featuers so we can pull later in feature addition\n",
        "    see notes next to add feature in LottFSNN file\"\"\"\n",
        "\n",
        "    Correlations = Correlations.sort_values(by = ['Corr'], ascending = False, inplace = False)\n",
        "\n",
        "    Correlations.reset_index(drop = True, inplace = True)\n",
        "\n",
        "    #merge Correlations with All_Terms to get list of effects which make up each term\n",
        "    #this list will be passed to actively select the terms we want in each iteration\n",
        "\n",
        "    Correlations = Correlations.merge(All_Terms, how = 'left', on = \"Feature\")\n",
        "\n",
        "    MECorr = Feature_OHE_Data.corr()\n",
        "\n",
        "    #drop Y from interactions data to use in creating feature training data\n",
        "    #cant drop before this since we use Y in interactions data for correlations\n",
        "    Interactions_Data.drop('Y', axis = 1, inplace = True)\n",
        "\n",
        "    ###########################################################\n",
        "\n",
        "    #Standardize Feature Data\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    Standard_Feature_Array = scaler.fit_transform(Interactions_Data)\n",
        "\n",
        "\n",
        "\n",
        "    #may also want to normalize response data and compare\n",
        "    #standardize resposne data\n",
        "    if Response_Data['Y'].dtype == 'float64':\n",
        "        Response_Data = pd.DataFrame(scaler.fit_transform(Response_Data))\n",
        "        Response_Data.columns = ['Y']\n",
        "\n",
        "    #now that we have data we can create a train/validation split\n",
        "    #we have no need for test data here since we do not care about our models predictive performance\n",
        "\n",
        "    Train_Feature_Data, Val_Feature_Data, Train_Response_Data, Val_Response_Data = splitData(Standard_Feature_Array = Standard_Feature_Array, Response_Data = Response_Data)\n",
        "\n",
        "    #update column names\n",
        "    Train_Feature_Data.columns = Interactions_Data.columns\n",
        "    Val_Feature_Data.columns = Interactions_Data.columns\n",
        "\n",
        "    #Response_Data[0].dtype\n",
        "\n",
        "    #check response type\n",
        "    #if its continuous we'll standardize it\n",
        "    #if its categorical we'll create one hot vectors\n",
        "\n",
        "    #THIS ASSUMES ANY INTEGER RESPONSE WILL BE A CATEGORICAL VARIABLE\n",
        "    #NUMBERS WITHOUT DECIMALS WOULD BE TREATED AS CATEGORICAL WHICH IS NOT GREAT\n",
        "    #TO BE FAIR MOST PACKAGES ARE NOT INTUITIVE ABOUT THIS EITHER\n",
        "    #SHOULD GO BACK AND UPDATE FOR USABILITY\n",
        "\n",
        "    if Train_Response_Data['Y'].dtype == 'int32' or Train_Response_Data['Y'].dtype == 'int64' or Train_Response_Data['Y'].dtype == 'object':\n",
        "        Train_Response_Vector = pd.get_dummies(Train_Response_Data['Y'].values)\n",
        "        Train_Response_Vector = np.asmatrix(Train_Response_Vector)\n",
        "        Val_Response_Vector = pd.get_dummies(Val_Response_Data['Y'].values)\n",
        "        Val_Response_Vector = np.asmatrix(Val_Response_Vector)\n",
        "    else:\n",
        "        Train_Response_Vector = (np.asmatrix(Train_Response_Data['Y'].values)).transpose()\n",
        "        Val_Response_Vector = (np.asmatrix(Val_Response_Data['Y'].values)).transpose()\n",
        "\n",
        "####################################################################################\n",
        "\n",
        "\n",
        "    \"\"\"Search Strategy\"\"\"\n",
        "\n",
        "    \"\"\"Fabricate all interactions and squared terms, pre-process including one hots and standardization\n",
        "    then run correlation matrix including response. Check all features, interactions, and squared terms\n",
        "    against the response. If response is categorical we'll have to consider all levels of the response.\"\"\"\n",
        "\n",
        "\n",
        "    #import pandas as pd\n",
        "\n",
        "\n",
        "    \"\"\"Its a waste of memory / computational time to calculate squared terms for or interactions\n",
        "    between different levels for the same one hot encoded variable. We still want to capture interactions between one hots and\n",
        "    continuous variables. Not sure how we want to do this with the current code though. Its still\n",
        "    running pretty quick and wont hurt any of the results but we could probably speed it up a bit\n",
        "    here if we look more into this. But also to play devils advoate, taking the time to search through\n",
        "    and identify which variables we should calculate/keep may take longer than just multiplying some\n",
        "    ones and zeros...\"\"\"\n",
        "\n",
        "\n",
        "    \"\"\"NEED TO FIGURE OUT HOW TO CAPTURE FEATURE NAME, TYPE, MAINEFFECTS, AND CORR\n",
        "    IN ONE DATAFRAME. MIGHT USE A DICT. USING _TERMS FOR TERM NAMES AND INTERACTIONS_DATA\n",
        "    FOR THE CORRELATIONS. THE CORRELATIONS ARE WORKING. THE TERMS ARE NOT\"\"\"\n",
        "\n",
        "    #make Interactions_Data which will contain the numeric values for our interactions\n",
        "    #these numeric values will be used to calculate correlations\n",
        "\n",
        "    #make _Terms which will capture the names and associated features for all...\n",
        "    #...interactions/maineffects/square terms\n",
        "\n",
        "\n",
        "\n",
        "    return Full_Data, MECorr, Correlations, Train_Feature_Data, Val_Feature_Data, Train_Response_Data, Val_Response_Data, Train_Response_Vector, Val_Response_Vector\n",
        "\n",
        "\n",
        "#############################################################################\n",
        "\n",
        "\"\"\"INSERT FSNN CODE\"\"\"\n",
        "\n",
        "##############################################################################\n",
        "def LottFSNN(Full_Data, MECorr, Correlations, Train_Feature_Data, Val_Feature_Data, Train_Response_Data, Val_Response_Data, Train_Response_Vector, Val_Response_Vector, **kwargs):\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    #import tensorflow as tf\n",
        "    #from tensorflow import keras\n",
        "    #from tensorflow.keras import layers\n",
        "\n",
        "    from keras.models import Model\n",
        "    #from keras.models import Sequential #uncomment if we use sequential method\n",
        "    from keras.optimizers import SGD\n",
        "    from keras.layers import Dense\n",
        "    from keras.layers import Input\n",
        "    from keras.utils import to_categorical\n",
        "\n",
        "    if Train_Response_Data['Y'].dtype == 'int32' or Train_Response_Data['Y'].dtype == 'int64' or Train_Response_Data['Y'].dtype == 'object':\n",
        "        eta = kwargs.get('eta', 10)\n",
        "        eta = int(eta)\n",
        "        epsilon = kwargs.get('epsilon', 10)\n",
        "        epsilon = int(epsilon)\n",
        "        delta = kwargs.get('delta', 3)\n",
        "        delta = int(delta)\n",
        "        xi = kwargs.get('xi', 0.8)\n",
        "        xi = float(xi)\n",
        "        alpha = kwargs.get('alpha', 0.95)\n",
        "        alpha = float(alpha)\n",
        "        phi = kwargs.get('phi', 0.9)\n",
        "        phi = float(phi)\n",
        "        omega = kwargs.get('omega', 0.01)\n",
        "        omega = float(omega)\n",
        "    #else continuous\n",
        "    else:\n",
        "        eta = kwargs.get('eta', 10)\n",
        "        eta = int(eta)\n",
        "        epsilon = kwargs.get('epsilon', 10)\n",
        "        epsilon = int(epsilon)\n",
        "        delta = kwargs.get('delta', 3)\n",
        "        delta = int(delta)\n",
        "        xi = kwargs.get('xi', 0.8)\n",
        "        xi = float(xi)\n",
        "        alpha = kwargs.get('alpha', 0.02)\n",
        "        alpha = float(alpha)\n",
        "        phi = kwargs.get('phi', 1.1)\n",
        "        phi = float(phi)\n",
        "        omega = kwargs.get('omega', 0.01)\n",
        "        omega = float(omega)\n",
        "\n",
        "\n",
        "    #######################################################\n",
        "    \"\"\"Count features/responses\"\"\"\n",
        "\n",
        "\n",
        "    #can use import math then math.comb in python version 3.8+\n",
        "    #were in python 3.7 so well just define comb manually\n",
        "    def comb(N,k):\n",
        "        import math\n",
        "        f = math.factorial\n",
        "        amount = f(N) // (f(k) * f(N-k))\n",
        "        return amount\n",
        "\n",
        "\n",
        "    def countfeatures(Data):\n",
        "        #capture our number of features/interactions/response\n",
        "        #maybe go back and add cap at 1225 interactions\n",
        "        #see notes\n",
        "        #it didnt work well\n",
        "        #see experiment 2 v 2\n",
        "        num_features = len(Data.columns)\n",
        "        if num_features > 5:\n",
        "            num_interactions = comb(num_features, 2)\n",
        "        else:\n",
        "            #this says num_interactions but it really represents the minimum number of nodes in our network\n",
        "            num_interactions = eta\n",
        "        return num_features, num_interactions\n",
        "\n",
        "    #########################################################\n",
        "    \"\"\"Build NN Format\"\"\"\n",
        "\n",
        "    def buildNN(num_features, num_interactions, num_response, act_fun, out_act_fun):\n",
        "\n",
        "        inputs = Input(shape=(num_features,))\n",
        "        x = Dense(num_features, activation= act_fun)(inputs)\n",
        "        x = Dense(num_interactions, activation= act_fun)(x)\n",
        "        x = Dense(num_interactions, activation= act_fun)(x)\n",
        "        outputs = Dense(num_response, activation= out_act_fun)(x)\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "        return model\n",
        "\n",
        "    #########################################################\n",
        "\n",
        "    \"\"\"Iterate features through NN\"\"\"\n",
        "\n",
        "    #########################################################\n",
        "\n",
        "    \"\"\"Only need to do these steps once\"\"\"\n",
        "    #ADJUSTING TO 15 TO TEST AGAINST RFE\n",
        "    num_epochs = epsilon\n",
        "\n",
        "    #only need to capture num_response once\n",
        "    num_response = np.shape(Train_Response_Vector)[1]\n",
        "\n",
        "    \"\"\"Define acc and loss metrics\"\"\"\n",
        "    #continuous standardized response\n",
        "    if Train_Response_Data['Y'].dtype == 'float64':\n",
        "        act_fun = 'relu' #should also try sigmoid\n",
        "        out_act_fun = 'linear' #will try norm and relu later\n",
        "        nn_loss = 'mean_squared_error'\n",
        "        nn_acc = 'mean_squared_error'\n",
        "\n",
        "    #categorical multi-category response\n",
        "    if Train_Response_Data['Y'].dtype == 'int32' or Train_Response_Data['Y'].dtype == 'int64' or Train_Response_Data['Y'].dtype == 'object':\n",
        "        if num_response > 1:\n",
        "            act_fun = 'relu' #should also try sigmoid\n",
        "            out_act_fun = 'softmax'\n",
        "            nn_loss = 'categorical_crossentropy'\n",
        "            nn_acc = 'accuracy'\n",
        "\n",
        "    #see notes in draft for binary options.\n",
        "\n",
        "    #########################################################\n",
        "\n",
        "    #establish lists for tracking, run_count, features, acc\n",
        "\n",
        "    Run_List = []\n",
        "    Feature_List = []\n",
        "    Acc_List = []\n",
        "\n",
        "\n",
        "    ##########################################################\n",
        "\n",
        "    \"\"\"Begin baseline\"\"\"\n",
        "    \"\"\"Baseline will train on all main effects\"\"\"\n",
        "\n",
        "    Main_Effects_List = list(MECorr.columns)\n",
        "\n",
        "    num_features, num_interactions = countfeatures(Train_Feature_Data[Main_Effects_List])\n",
        "\n",
        "    model = buildNN(num_features = num_features, num_interactions = num_interactions, num_response = num_response, act_fun = act_fun, out_act_fun = out_act_fun)\n",
        "\n",
        "    model_keep = buildNN(num_features = num_features, num_interactions = num_interactions, num_response = num_response, act_fun = act_fun, out_act_fun = out_act_fun)\n",
        "\n",
        "\n",
        "    # complile the model with loss = binary_crossentropy\n",
        "    model.compile(optimizer= SGD(learning_rate=.01),\n",
        "                  loss= nn_loss,\n",
        "                  metrics=[nn_acc])\n",
        "\n",
        "    model_keep.compile(optimizer= SGD(learning_rate=.01),\n",
        "                  loss= nn_loss,\n",
        "                  metrics=[nn_acc])\n",
        "\n",
        "    #train NN for some number of epochs\n",
        "    history_baseline = model.fit(x=Train_Feature_Data[Main_Effects_List],y=Train_Response_Vector, batch_size=1, epochs=num_epochs, verbose= 1, callbacks=None,\n",
        "                        validation_data=(Val_Feature_Data[Main_Effects_List],Val_Response_Vector), shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0)\n",
        "\n",
        "    Baseline_Data = pd.DataFrame(history_baseline.history)\n",
        "\n",
        "    # history_keep = model_keep.fit(x=Train_Feature_Data[Main_Effects_List],y=Train_Response_Vector, batch_size=1, epochs=num_epochs, verbose= 1, callbacks=None,\n",
        "    #                     validation_data=(Val_Feature_Data[Main_Effects_List],Val_Response_Vector), shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0)\n",
        "\n",
        "    #Keep_Data = pd.DataFrame(history_keep.history)\n",
        "\n",
        "    # Test_Set = pd.read_csv('Test_Set.csv')\n",
        "    # Test_Set.drop('Cumulative_Error', axis = 1, inplace = True)\n",
        "\n",
        "    #Keep_Predictions = model_keep.predict(Test_Set)\n",
        "\n",
        "    #define acc_metric to capture from dataframe\n",
        "\n",
        "    #this wont be efficient to do every loop\n",
        "    #rethink this setup\n",
        "    if nn_acc == 'mean_squared_error':\n",
        "        val_acc_metric = 'val_mean_squared_error'\n",
        "        baseline_acc = Baseline_Data[val_acc_metric].agg(lambda grp: grp.nsmallest(3).mean())\n",
        "    else:\n",
        "        val_acc_metric = 'val_accuracy'\n",
        "        baseline_acc = Baseline_Data[val_acc_metric].agg(lambda grp: grp.nlargest(3).mean())\n",
        "\n",
        "    print('')\n",
        "    print('With Features:', Main_Effects_List)\n",
        "    print('Baseline Performance:', val_acc_metric, baseline_acc)\n",
        "\n",
        "    Run_List.append([1])\n",
        "    Feature_List.append(Main_Effects_List)\n",
        "    Acc_List.append([baseline_acc])\n",
        "\n",
        "    #assign best_acc to our baseline_acc\n",
        "\n",
        "    best_acc = baseline_acc\n",
        "    ###############################################################################\n",
        "\n",
        "    \"\"\"Begin first loop adding from Correlations\"\"\"\n",
        "\n",
        "    #will need to loop over .iloc[i] while we add terms\n",
        "    #this is giving us the main effects for the term with the highest correlation which may be an interaction or squared term\n",
        "    #see ['Feature'] for the feature with the highest correlations\n",
        "\n",
        "    Salient_Feature_Train_Data = pd.DataFrame(Train_Feature_Data[Correlations.iloc[0]['Feature']])\n",
        "    Salient_Feature_Val_Data = pd.DataFrame(Val_Feature_Data[Correlations.iloc[0]['Feature']])\n",
        "    Current_Feature_List = list(Salient_Feature_Train_Data.columns)\n",
        "\n",
        "    if Correlations.iloc[0]['Type'] == \"Interaction\":\n",
        "        ffcorr = MECorr.iloc[MECorr.columns.get_loc(Correlations.iloc[0]['MainEffects'][0])][MECorr.columns.get_loc(Correlations.iloc[0]['MainEffects'][1])]\n",
        "        if ffcorr < xi:\n",
        "            for x in Correlations.iloc[0]['MainEffects']:\n",
        "                if x not in Current_Feature_List:\n",
        "                    Current_Feature_List.append(x)\n",
        "        else:\n",
        "            Current_Feature_List.append(Correlations.iloc[0]['MainEffects'][0])\n",
        "    else:\n",
        "        for x in Correlations.iloc[0]['MainEffects']:\n",
        "                if x not in Current_Feature_List:\n",
        "                    Current_Feature_List.append(x)\n",
        "\n",
        "    Salient_Feature_Train_Data = Train_Feature_Data[Current_Feature_List]\n",
        "    Salient_Feature_Val_Data = Val_Feature_Data[Current_Feature_List]\n",
        "\n",
        "\n",
        "    num_features, num_interactions = countfeatures(Salient_Feature_Train_Data)\n",
        "\n",
        "    model = buildNN(num_features = num_features, num_interactions = num_interactions, num_response = num_response, act_fun = act_fun, out_act_fun = out_act_fun)\n",
        "\n",
        "    # complile the model with loss = binary_crossentropy\n",
        "    model.compile(optimizer= SGD(learning_rate=.01),\n",
        "                  loss= nn_loss,\n",
        "                  metrics=[nn_acc])\n",
        "\n",
        "    #train NN for some number of epochs\n",
        "    history_firstrun = model.fit(x=Salient_Feature_Train_Data,y=Train_Response_Vector, batch_size=1, epochs=num_epochs, verbose= 1, callbacks=None,\n",
        "                        validation_data=(Salient_Feature_Val_Data,Val_Response_Vector), shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0)\n",
        "\n",
        "    Firstrun_Data = pd.DataFrame(history_firstrun.history)\n",
        "\n",
        "    #define acc_metric to capture from dataframe\n",
        "\n",
        "    #this wont be efficient to do every loop\n",
        "    #rethink this setup\n",
        "    if nn_acc == 'mean_squared_error':\n",
        "        val_acc_metric = 'val_mean_squared_error'\n",
        "        firstrun_acc = Firstrun_Data[val_acc_metric].agg(lambda grp: grp.nsmallest(delta).mean())\n",
        "    else:\n",
        "        val_acc_metric = 'val_accuracy'\n",
        "        firstrun_acc = Firstrun_Data[val_acc_metric].agg(lambda grp: grp.nlargest(delta).mean())\n",
        "\n",
        "    print('')\n",
        "    print('With Features:', list(Salient_Feature_Train_Data.columns))\n",
        "    print('Firstrun Performance:', val_acc_metric, firstrun_acc)\n",
        "\n",
        "    Run_List.append([2])\n",
        "    Feature_List.append(list(Salient_Feature_Train_Data.columns))\n",
        "    Acc_List.append([firstrun_acc])\n",
        "\n",
        "    if nn_acc == 'mean_squared_error':\n",
        "        if firstrun_acc < best_acc:\n",
        "            best_acc = firstrun_acc\n",
        "    else:\n",
        "        if firstrun_acc > best_acc:\n",
        "            best_acc = firstrun_acc\n",
        "\n",
        "    ########################################################################\n",
        "\n",
        "    \"\"\"Automate Starting On Second Run\"\"\"\n",
        "\n",
        "    #define function to add new features from correlation list\n",
        "    #will add single or group of features based on interaction\n",
        "\n",
        "    \"\"\"Add logic to take correlation of new feature with existing features in the model\n",
        "    check the difference between this value and its correlation with the response\"\"\"\n",
        "\n",
        "    #VERSION 1\n",
        "    def addnewfeature():\n",
        "        Current_Feature_List = list(Salient_Feature_Train_Data.columns)\n",
        "\n",
        "        num_new_features = 0\n",
        "        num_current_features = len(Current_Feature_List)\n",
        "\n",
        "        #while num_new_features < 1:\n",
        "        for i in range(1,len(Correlations)):\n",
        "            num_new_features = len(Current_Feature_List) - num_current_features\n",
        "            if num_new_features > 0:\n",
        "                if Correlations.iloc[i-1]['Feature'] not in Current_Feature_List:\n",
        "                    Current_Feature_List.append(Correlations.iloc[i-1]['Feature'])\n",
        "                break\n",
        "            else:\n",
        "                for x in Correlations.iloc[i]['MainEffects']:\n",
        "                    if x not in Current_Feature_List:\n",
        "                        #add another if statment representing difference between\n",
        "                        #worst_feature_feature correlation and feature_response correlation\n",
        "                        #max(MECorr.iloc[MECorr.columns.get_loc('XTWO')][['XTWO','XONE','XTHREE','XSIX']])\n",
        "                        ffcorr = max(MECorr.iloc[MECorr.columns.get_loc(x)][list(set(Current_Feature_List).intersection(Main_Effects_List))])\n",
        "                        if ffcorr < xi: #dont add features if they are highly correlated with any feature in our current feature set\n",
        "                            Current_Feature_List.append(x)\n",
        "\n",
        "        return Current_Feature_List\n",
        "\n",
        "    # #VERSION 2\n",
        "    # def addnewfeature():\n",
        "    #     num_new_features = 0\n",
        "    #     num_current_features = len(Current_Feature_List)\n",
        "\n",
        "    #     #while num_new_features < 1:\n",
        "    #     for i in range(1,len(Correlations)):\n",
        "    #         num_new_features = len(Current_Feature_List) - num_current_features\n",
        "\n",
        "    #         if num_new_features > 0:\n",
        "    #             break\n",
        "\n",
        "    #         #maintaining model hierarchy by adding the main effect for a square\n",
        "    #         #as well as the square will keep us from missing it later due to\n",
        "    #         #our correlation check and shouldnt hurt our answer\n",
        "    #         if Correlations.iloc[i]['Type'] == 'Square':\n",
        "    #             x = Correlations.iloc[i]['MainEffects']\n",
        "    #             if f'{x[0]}_{x[0]}' not in Current_Feature_List:\n",
        "    #                 Current_Feature_List.append(f'{x[0]}_{x[0]}')\n",
        "    #                 if x not in Current_Feature_List:\n",
        "    #                     Current_Feature_List.append(x[0])\n",
        "    #                 #now this is a little different\n",
        "    #                 #we are adding columns to our training data which\n",
        "    #                 #were not used in our baseline, this means our baseline may not\n",
        "    #                 #be representative but it should still improve our performance\n",
        "    #                 #in the case in which we have a quadratic effect present\n",
        "    #                 Train_Feature_Data[f'{x[0]}_{x[0]}'] = Train_Feature_Data[x]**2\n",
        "    #                 Val_Feature_Data[f'{x[0]}_{x[0]}'] = Val_Feature_Data[x]**2\n",
        "    #                 Salient_Feature_Train_Data = Train_Feature_Data[Current_Feature_List]\n",
        "    #                 Salient_Feature_Val_Data = Val_Feature_Data[Current_Feature_List]\n",
        "\n",
        "\n",
        "    #         else:\n",
        "    #             for x in Correlations.iloc[i]['MainEffects']:\n",
        "    #                 if x not in Current_Feature_List:\n",
        "    #                     #add another if statment representing difference between\n",
        "    #                     #worst_feature_feature correlation and feature_response correlation\n",
        "    #                     #max(MECorr.iloc[MECorr.columns.get_loc('XTWO')][['XTWO','XONE','XTHREE','XSIX']])\n",
        "    #                     ffcorr = max(MECorr.iloc[MECorr.columns.get_loc(x)][Current_Feature_List])\n",
        "    #                     if ffcorr < 0.8: #dont add features if they are highly correlated with any feature in our current feature set\n",
        "    #                         Current_Feature_List.append(x)\n",
        "    #                         Salient_Feature_Train_Data = Train_Feature_Data[Current_Feature_List]\n",
        "    #                         Salient_Feature_Val_Data = Val_Feature_Data[Current_Feature_List]\n",
        "\n",
        "\n",
        "    #     return Current_Feature_List, Salient_Feature_Train_Data, Salient_Feature_Val_Data\n",
        "\n",
        "\n",
        "    #LOOP\n",
        "    #THESE STEPS SEEM TO WORK WELL\n",
        "    #JUST NEED TO SET UP IN LOOP\n",
        "\n",
        "    Stopping_Criteria = False #stopping criteria to exit algorithm\n",
        "    Degraded_Performance = False #marker if we got worse than our best assuming our best is not our baseline\n",
        "    #will use degraded performance to back track to correct set if True\n",
        "    run_count = 3\n",
        "\n",
        "    if nn_acc == 'mean_squared_error':\n",
        "        if firstrun_acc < alpha:\n",
        "            Stopping_Criteria = True\n",
        "\n",
        "    if nn_acc == 'accuracy':\n",
        "        #ADJUSTING TO ADJUST ALPHA\n",
        "        if firstrun_acc > alpha:\n",
        "            Stopping_Criteria = True\n",
        "\n",
        "    run_acc = 0.5\n",
        "    #assign run_acc to 0.5 because it wont cause the loop to quit...\n",
        "    #...for regression or classification problems on the first loop\n",
        "\n",
        "    \"\"\"BEGIN LOOP\"\"\"\n",
        "\n",
        "    for i in range(1,len(Correlations)):\n",
        "\n",
        "        if nn_acc == 'mean_squared_error':\n",
        "            if run_acc < alpha:\n",
        "                Stopping_Criteria = True\n",
        "\n",
        "        if nn_acc == 'accuracy':\n",
        "            #UPDATING THIS FROM 0.95 TO 0.96 TO TEST BREAST CANCER ASSERTION\n",
        "            if run_acc > alpha:\n",
        "                Stopping_Criteria = True\n",
        "\n",
        "        #old method\n",
        "        #if nn_acc == 'mean_squared_error':\n",
        "        #    if best_acc != baseline_acc:\n",
        "        #        if best_acc < 0.01:\n",
        "        #            Stopping_Criteria = True\n",
        "\n",
        "        #old method\n",
        "        #if nn_acc == 'accuracy':\n",
        "        #    if best_acc != baseline_acc: #in the event the baseline and first run both reach 1.0 acc then this doesnt work and will keep running\n",
        "        #        if best_acc > 0.95:\n",
        "        #            Stopping_Criteria = True\n",
        "\n",
        "        if Stopping_Criteria == True:\n",
        "            break\n",
        "\n",
        "        #USE IF USING addnewfeature VERSION 1\n",
        "        Current_Feature_List = addnewfeature()\n",
        "\n",
        "        #USE IF USING addnewfeature VERSION 1\n",
        "        Salient_Feature_Train_Data = Train_Feature_Data[Current_Feature_List]\n",
        "        Salient_Feature_Val_Data = Val_Feature_Data[Current_Feature_List]\n",
        "\n",
        "        #USE IF USING addnewfeature VERSION 2\n",
        "        #Current_Feature_List, Salient_Feature_Train_Data, Salient_Feature_Val_Data = addnewfeature()\n",
        "\n",
        "        num_features, num_interactions = countfeatures(Salient_Feature_Train_Data)\n",
        "\n",
        "        model = buildNN(num_features = num_features, num_interactions = num_interactions, num_response = num_response, act_fun = act_fun, out_act_fun = out_act_fun)\n",
        "\n",
        "        # complile the model with loss = binary_crossentropy\n",
        "        model.compile(optimizer= SGD(learning_rate=.01),\n",
        "                      loss= nn_loss,\n",
        "                      metrics=[nn_acc])\n",
        "\n",
        "        history = model.fit(x=Salient_Feature_Train_Data,y=Train_Response_Vector, batch_size=1, epochs=num_epochs, verbose= 1, callbacks=None,\n",
        "                            validation_data=(Salient_Feature_Val_Data,Val_Response_Vector), shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0)\n",
        "\n",
        "        History_Data = pd.DataFrame(history.history)\n",
        "\n",
        "        if nn_acc == 'mean_squared_error':\n",
        "            val_acc_metric = 'val_mean_squared_error'\n",
        "            run_acc = History_Data[val_acc_metric].agg(lambda grp: grp.nsmallest(delta).mean())\n",
        "        else:\n",
        "            val_acc_metric = 'val_accuracy'\n",
        "            run_acc = History_Data[val_acc_metric].agg(lambda grp: grp.nlargest(delta).mean())\n",
        "\n",
        "        #update this to capture in list to be converted into datafarme\n",
        "        print('')\n",
        "        print('With Features:', Current_Feature_List)\n",
        "        print('Run ',run_count,' Performance:', val_acc_metric, run_acc)\n",
        "\n",
        "        Run_List.append([run_count])\n",
        "        Feature_List.append(Current_Feature_List)\n",
        "        Acc_List.append([run_acc])\n",
        "\n",
        "        #update best run and stopping criteria\n",
        "        if nn_acc == 'mean_squared_error': #if numeric response\n",
        "            if run_acc < best_acc: #if this run is better than our best\n",
        "                best_acc = run_acc #establish as new best\n",
        "            elif run_acc > best_acc: #if this run is worse than our best\n",
        "                if best_acc < baseline_acc: #if we've improved since baseline\n",
        "                    if run_acc > best_acc: #if this run is worse than our best\n",
        "                        Stopping_Criteria = True #stop\n",
        "                        Degraded_Performance = True\n",
        "                elif best_acc == baseline_acc:#otherwise if our baseline is our best\n",
        "                    if run_acc < (phi * best_acc):#if we have built close to baseline acc\n",
        "                        Stopping_Criteria = True#stop\n",
        "        else: #if categorical response\n",
        "            if run_acc > best_acc: #if this run is better than our best\n",
        "                best_acc = run_acc #establish as new best\n",
        "            elif run_acc < best_acc: #if this run is worse than our best\n",
        "                if best_acc > baseline_acc: #if we've improved since baseline\n",
        "                    if run_acc < best_acc: #if this run is worse than our best\n",
        "                        Stopping_Criteria = True #stop\n",
        "                        Degraded_Performance = True\n",
        "                elif best_acc == baseline_acc: #if baseline is our best\n",
        "                    #UPDATING THIS FROM 0.9 TO 1.0 TO TEST BREAST CANCER ASSERTION\n",
        "                    if run_acc > (phi * best_acc): #if we have built close to baseline acc\n",
        "                        Stopping_Criteria = True #stop\n",
        "\n",
        "        run_count +=1\n",
        "        i +=1\n",
        "\n",
        "\n",
        "    All_Runs_Data = pd.DataFrame()\n",
        "    All_Runs_Data['Run'] = Run_List\n",
        "    All_Runs_Data['Features'] = Feature_List\n",
        "\n",
        "    if nn_acc == 'mean_squared_error':\n",
        "        All_Runs_Data['Val_MSE'] = Acc_List\n",
        "    else:\n",
        "        All_Runs_Data['Val_Acc'] = Acc_List\n",
        "\n",
        "\n",
        "    print('')\n",
        "    print('All Runs')\n",
        "    print(All_Runs_Data)\n",
        "    #ran into a case where we did degraded but by less than omega (0.01 by default) and the code throws an error of course need to program some type of warning here\n",
        "    if Degraded_Performance == False:\n",
        "        Final_Features_List = list(set(All_Runs_Data.iloc[-1]['Features']).intersection(Main_Effects_List))\n",
        "    else:\n",
        "        for i in range(-2,-len(All_Runs_Data),-1):\n",
        "            if nn_acc == 'mean_squared_error':\n",
        "                check_improvement = All_Runs_Data.iloc[i-1]['Val_MSE'][0] - All_Runs_Data.iloc[i]['Val_MSE'][0]\n",
        "                if check_improvement > omega:\n",
        "                    last_improvement_index = i\n",
        "                    break\n",
        "            if nn_acc == 'accuracy':\n",
        "                check_improvement = All_Runs_Data.iloc[i]['Val_Acc'][0] - All_Runs_Data.iloc[i-1]['Val_Acc'][0]\n",
        "                if check_improvement > omega:\n",
        "                    last_improvement_index = i\n",
        "                    break\n",
        "        Final_Features_List = list(set(All_Runs_Data.iloc[last_improvement_index]['Features']).intersection(Main_Effects_List))\n",
        "\n",
        "    return All_Runs_Data, Final_Features_List\n",
        "# \"\"\"Need to introduce K-fold by running maybe 10 times and capturing last run\n",
        "# last run will work since we'll always add features in the same order\n",
        "# as long as your run was ran maybe 5 or more times then we pick you.\n",
        "# This is another hyper parameter. Maybe describe as confidence for inclusion\n",
        "# out of 10 runs. Let the user choose the number of runs and confidence.\"\"\"\n",
        "\n",
        "\"\"\"********************END FSNN CODE*********************\"\"\"\n",
        "\n",
        "########################################################################\n",
        "\n",
        "\"\"\"GRAFS PUT IT ALL TOGETHER\"\"\"\n",
        "\n",
        "########################################################################\n",
        "\n",
        "def GRAFS(filepath, contains_response, perform_rfe, **kwargs):\n",
        "    \"\"\"\n",
        "    BUILD DOC STRING\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filepath : TYPE\n",
        "        DESCRIPTION.\n",
        "    contains_response : TYPE\n",
        "        DESCRIPTION.\n",
        "    perform_rfe : TYPE\n",
        "        DESCRIPTION.\n",
        "    **kwargs : TYPE\n",
        "        DESCRIPTION.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    #establish optional arguments through pre-processing\n",
        "    response_name = kwargs.get('response_name', None)\n",
        "    rfe_retain = kwargs.get('rfe_retain', 12)\n",
        "    rfe_retain = int(rfe_retain)\n",
        "    rfe_step = kwargs.get('rfe_step', 1)\n",
        "    rfe_step = int(rfe_step)\n",
        "\n",
        "\n",
        "    #PERFORM PRE-PROCESSING: This includes K-means and RFE as appropriate\n",
        "    Full_Data, MECorr, Correlations, Train_Feature_Data, Val_Feature_Data, Train_Response_Data, Val_Response_Data, Train_Response_Vector, Val_Response_Vector = FSPreProcessing(filepath = filepath, contains_response = contains_response, response_name = response_name, perform_rfe = perform_rfe, rfe_retain = rfe_retain, rfe_step = rfe_step)\n",
        "\n",
        "\n",
        "    #establish option arguments for NN: remember 2 sets based on problem type\n",
        "    #reference Table 2 Hyper-parameter Settings in publication\n",
        "\n",
        "    #if categorical\n",
        "    if Train_Response_Data['Y'].dtype == 'int32' or Train_Response_Data['Y'].dtype == 'int64' or Train_Response_Data['Y'].dtype == 'object':\n",
        "        eta = kwargs.get('eta', 10)\n",
        "        eta = int(eta)\n",
        "        epsilon = kwargs.get('epsilon', 10)\n",
        "        epsilon = int(epsilon)\n",
        "        delta = kwargs.get('delta', 3)\n",
        "        delta = int(delta)\n",
        "        xi = kwargs.get('xi', 0.8)\n",
        "        xi = float(xi)\n",
        "        alpha = kwargs.get('alpha', 0.95)\n",
        "        alpha = float(alpha)\n",
        "        phi = kwargs.get('phi', 0.9)\n",
        "        phi = float(phi)\n",
        "        omega = kwargs.get('omega', 0.01)\n",
        "        omega = float(omega)\n",
        "    #else continuous\n",
        "    else:\n",
        "        eta = kwargs.get('eta', 10)\n",
        "        eta = int(eta)\n",
        "        epsilon = kwargs.get('epsilon', 10)\n",
        "        epsilon = int(epsilon)\n",
        "        delta = kwargs.get('delta', 3)\n",
        "        delta = int(delta)\n",
        "        xi = kwargs.get('xi', 0.8)\n",
        "        xi = float(xi)\n",
        "        alpha = kwargs.get('alpha', 0.02)\n",
        "        alpha = float(alpha)\n",
        "        phi = kwargs.get('phi', 1.1)\n",
        "        phi = float(phi)\n",
        "        omega = kwargs.get('omega', 0.01)\n",
        "        omega = float(omega)\n",
        "\n",
        "    Full_Progress, Salient_Features = LottFSNN(Full_Data, MECorr, Correlations, Train_Feature_Data, Val_Feature_Data, Train_Response_Data, Val_Response_Data, Train_Response_Vector, Val_Response_Vector, eta = eta, epsilon = epsilon, delta = delta, xi = xi, alpha = alpha, phi = phi, omega = omega)\n",
        "\n",
        "    return Full_Progress, Salient_Features\n",
        "\n",
        "#############################################################################\n",
        "\n",
        "#\"\"\"END GRAFS CODE\"\"\"\n",
        "\n",
        "#############################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Execute GRAFS\n",
        "\n",
        "The next cell executes the method, saves individual run performances, saves salient features, and prints salient features"
      ],
      "metadata": {
        "id": "tOevSCkJgQ4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ourfilepath = \"/content/drive/MyDrive/ENOSE_Train_Data_Values_Ready.csv\"\n",
        "ourfilepath = \"https://raw.githubusercontent.com/levilott/GRAFS/main/breast_cancer.csv\"\n",
        "\n",
        "Full_Progress, Salient_Features = GRAFS(filepath = ourfilepath, contains_response = True, response_name = 'label', perform_rfe = True, rfe_retain = 10, epsilon = 10, alpha = 0.95, phi = 0.95)\n",
        "\n",
        "Full_Progress.to_csv(\"GRAFS_Results.csv\")\n",
        "import pandas as pd\n",
        "pd.DataFrame(Salient_Features).to_csv('GRAFS_Salient_Features.csv', index = False)\n",
        "\n",
        "Salient_Features"
      ],
      "metadata": {
        "id": "x6K_-0UiCYRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3de10cb6-8cf3-4844-c0ac-28fd421d343f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7684 - loss: 0.5110 - val_accuracy: 0.9518 - val_loss: 0.1511\n",
            "Epoch 2/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9470 - loss: 0.1710 - val_accuracy: 0.9781 - val_loss: 0.1005\n",
            "Epoch 3/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9680 - loss: 0.0931 - val_accuracy: 0.9737 - val_loss: 0.0797\n",
            "Epoch 4/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9567 - loss: 0.0977 - val_accuracy: 0.9693 - val_loss: 0.0712\n",
            "Epoch 5/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9833 - loss: 0.0735 - val_accuracy: 0.9693 - val_loss: 0.0662\n",
            "Epoch 6/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9745 - loss: 0.0828 - val_accuracy: 0.9693 - val_loss: 0.0619\n",
            "Epoch 7/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9887 - loss: 0.0488 - val_accuracy: 0.9737 - val_loss: 0.0620\n",
            "Epoch 8/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9725 - loss: 0.0765 - val_accuracy: 0.9737 - val_loss: 0.0625\n",
            "Epoch 9/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9780 - loss: 0.0571 - val_accuracy: 0.9781 - val_loss: 0.0597\n",
            "Epoch 10/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9817 - loss: 0.0757 - val_accuracy: 0.9737 - val_loss: 0.0601\n",
            "\n",
            "With Features: ['mean concave points', 'area error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points']\n",
            "Baseline Performance: val_accuracy 0.9766081968943278\n",
            "Epoch 1/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8405 - loss: 0.5293 - val_accuracy: 0.8991 - val_loss: 0.3604\n",
            "Epoch 2/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9203 - loss: 0.3260 - val_accuracy: 0.9079 - val_loss: 0.2867\n",
            "Epoch 3/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9157 - loss: 0.2641 - val_accuracy: 0.8904 - val_loss: 0.2594\n",
            "Epoch 4/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8977 - loss: 0.2584 - val_accuracy: 0.8904 - val_loss: 0.2481\n",
            "Epoch 5/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8932 - loss: 0.2873 - val_accuracy: 0.9167 - val_loss: 0.2503\n",
            "Epoch 6/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9280 - loss: 0.2031 - val_accuracy: 0.8991 - val_loss: 0.2366\n",
            "Epoch 7/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8890 - loss: 0.2745 - val_accuracy: 0.8991 - val_loss: 0.2362\n",
            "Epoch 8/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9264 - loss: 0.1826 - val_accuracy: 0.9079 - val_loss: 0.2353\n",
            "Epoch 9/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8986 - loss: 0.2687 - val_accuracy: 0.9079 - val_loss: 0.2408\n",
            "Epoch 10/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9235 - loss: 0.2045 - val_accuracy: 0.8991 - val_loss: 0.2329\n",
            "\n",
            "With Features: ['worst concave points']\n",
            "Firstrun Performance: val_accuracy 0.9108187158902487\n",
            "Epoch 1/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.4726 - loss: 0.6844 - val_accuracy: 0.8860 - val_loss: 0.4499\n",
            "Epoch 2/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8994 - loss: 0.3676 - val_accuracy: 0.8991 - val_loss: 0.2790\n",
            "Epoch 3/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9168 - loss: 0.2751 - val_accuracy: 0.9035 - val_loss: 0.2646\n",
            "Epoch 4/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9245 - loss: 0.2177 - val_accuracy: 0.9035 - val_loss: 0.2364\n",
            "Epoch 5/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.1745 - val_accuracy: 0.9079 - val_loss: 0.2324\n",
            "Epoch 6/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.1889 - val_accuracy: 0.9079 - val_loss: 0.2320\n",
            "Epoch 7/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9473 - loss: 0.1630 - val_accuracy: 0.9079 - val_loss: 0.2344\n",
            "Epoch 8/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9249 - loss: 0.2160 - val_accuracy: 0.8947 - val_loss: 0.2273\n",
            "Epoch 9/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9323 - loss: 0.2156 - val_accuracy: 0.8947 - val_loss: 0.2373\n",
            "Epoch 10/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9296 - loss: 0.2001 - val_accuracy: 0.9079 - val_loss: 0.2318\n",
            "\n",
            "With Features: ['worst concave points', 'worst smoothness', 'worst smoothness_worst concave points']\n",
            "Run  3  Performance: val_accuracy 0.9078947305679321\n",
            "Epoch 1/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7876 - loss: 0.5140 - val_accuracy: 0.9474 - val_loss: 0.1642\n",
            "Epoch 2/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9612 - loss: 0.1450 - val_accuracy: 0.9518 - val_loss: 0.1373\n",
            "Epoch 3/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9539 - loss: 0.1317 - val_accuracy: 0.9474 - val_loss: 0.1351\n",
            "Epoch 4/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9514 - loss: 0.1149 - val_accuracy: 0.9474 - val_loss: 0.1318\n",
            "Epoch 5/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9349 - loss: 0.1577 - val_accuracy: 0.9474 - val_loss: 0.1294\n",
            "Epoch 6/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9497 - loss: 0.1246 - val_accuracy: 0.9474 - val_loss: 0.1295\n",
            "Epoch 7/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9753 - loss: 0.0767 - val_accuracy: 0.9474 - val_loss: 0.1247\n",
            "Epoch 8/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9494 - loss: 0.0894 - val_accuracy: 0.9474 - val_loss: 0.1231\n",
            "Epoch 9/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9568 - loss: 0.1113 - val_accuracy: 0.9474 - val_loss: 0.1254\n",
            "Epoch 10/10\n",
            "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9636 - loss: 0.1012 - val_accuracy: 0.9474 - val_loss: 0.1260\n",
            "\n",
            "With Features: ['worst concave points', 'worst smoothness', 'worst smoothness_worst concave points', 'worst radius', 'worst radius_worst concave points']\n",
            "Run  4  Performance: val_accuracy 0.9488304257392883\n",
            "\n",
            "All Runs\n",
            "   Run                                           Features  \\\n",
            "0  [1]  [mean concave points, area error, worst radius...   \n",
            "1  [2]                             [worst concave points]   \n",
            "2  [3]  [worst concave points, worst smoothness, worst...   \n",
            "3  [4]  [worst concave points, worst smoothness, worst...   \n",
            "\n",
            "                Val_Acc  \n",
            "0  [0.9766081968943278]  \n",
            "1  [0.9108187158902487]  \n",
            "2  [0.9078947305679321]  \n",
            "3  [0.9488304257392883]  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['worst radius', 'worst smoothness', 'worst concave points']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}